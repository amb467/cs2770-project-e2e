[EVAL]
Encoder_path = models/E2.ckpt
Decoder_path = models/D2.ckpt
vocab_path = data/vocab.pkl
embed_size = 256
hidden_size = 512
num_layers = 1
crop_size = 224
image_dir = data/val_resized2014
caption_path = data/annotations/captions_val2014.json
num_workers = 2
batch_size = 1

[EVAL2]
Encoder_path = models/v3_encoder-5-1000.ckpt
Decoder_path = models/v3_decoder-5-1000.ckpt
vocab_path = data/vocab.pkl
embed_size = 300
hidden_size = 50
num_layers = 3
crop_size = 224
image_dir = data/val_resized2014
caption_path = data/annotations/captions_val2014.json
num_workers = 2
batch_size = 1

[EVAL3]
Encoder_path = models/v3_encoder-1-3000.ckpt
Decoder_path = models/v3_decoder-1-3000.ckpt
vocab_path = data/vocab.pkl
embed_size = 512
hidden_size = 512
num_layers = 1
crop_size = 299
image_dir = data/val_resized2014
caption_path = data/annotations/captions_val2014.json
num_workers = 2
batch_size = 1

[TRAIN-VQA]
model_path = models/vqa
Encoder_path = None
Decoder_path = None
crop_size = 299
image_dir = data/train
data_file_path = data/data_files/train_data.txt
vocab_path = data/vocab/vocab_vqa.pkl
log_step = 10
embed_size = 16
hidden_size = 16
num_layers = 1
num_epochs = 1
batch_size = 128
num_workers = 2
learning_rate = 0.0001

[TRAIN-VQG]
model_path = models/vqg
Encoder_path = None
Decoder_path = None
crop_size = 299
image_dir = data/train
data_file_path = data/data_files/train_data.txt
vocab_path = data/vocab/vocab_vqg.pkl
log_step = 10
embed_size = 16
hidden_size = 16
num_layers = 8
num_epochs = 2
batch_size = 128
num_workers = 2
learning_rate = 0.0001